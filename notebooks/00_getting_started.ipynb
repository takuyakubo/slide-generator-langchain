{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スライド生成入門\n",
    "\n",
    "このノートブックでは、LangChainとLangGraphを使用してPDFやHTMLドキュメントからスライドプレゼンテーションを生成する基本的な流れを紹介します。\n",
    "\n",
    "## 概要\n",
    "\n",
    "このプロセスは以下のステップで構成されています：\n",
    "\n",
    "1. ドキュメントの読み込みと前処理\n",
    "2. コンテンツ構造の抽出\n",
    "3. スライドアウトラインの生成\n",
    "4. 詳細なスライドコンテンツの生成\n",
    "5. HTMLスライドの作成\n",
    "\n",
    "それでは始めましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境のセットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルから環境変数を読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# プロジェクトルートへのパスを追加\n",
    "root_dir = Path().absolute().parent\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "# APIキーの確認\n",
    "required_keys = [\"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]\n",
    "missing_keys = [key for key in required_keys if os.getenv(key) is None]\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"警告: 次のAPIキーが設定されていません: {', '.join(missing_keys)}\")\n",
    "    print(\"プロジェクトルートの.envファイルにAPIキーを設定してください\")\n",
    "else:\n",
    "    print(\"APIキーが正常に設定されています\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain関連のインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.document_loaders import PyPDFLoader, UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "import langgraph.graph as lg\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# 使用するLLMの設定\n",
    "model_name = os.getenv(\"DEFAULT_MODEL\", \"claude-3-haiku-20240307\")\n",
    "\n",
    "if model_name.startswith(\"claude\"):\n",
    "    llm = ChatAnthropic(model=model_name)\n",
    "elif model_name.startswith(\"gpt\"):\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "else:\n",
    "    raise ValueError(f\"サポートされていないモデル: {model_name}\")\n",
    "\n",
    "print(f\"使用するモデル: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ドキュメント処理のユーティリティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=200):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "    def load_document(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"ファイルタイプに応じたローダーを使用\"\"\"\n",
    "        if file_path.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.html'):\n",
    "            loader = UnstructuredHTMLLoader(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"サポートされていないファイルタイプ: {file_path}\")\n",
    "        \n",
    "        return loader.load()\n",
    "    \n",
    "    def process_documents(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"ドキュメントをチャンクに分割\"\"\"\n",
    "        return self.text_splitter.split_documents(docs)\n",
    "    \n",
    "    def extract_toc(self, docs: List[Document]) -> List[str]:\n",
    "        \"\"\"目次を抽出する試み (簡易実装)\"\"\"\n",
    "        # 実際のプロジェクトではもっと洗練されたロジックが必要\n",
    "        potential_headings = []\n",
    "        for doc in docs:\n",
    "            lines = doc.page_content.split('\\n')\n",
    "            for line in lines:\n",
    "                # 見出しらしい行を抽出 (単純な例)\n",
    "                if line.strip() and len(line.strip()) < 100 and not line.endswith('.'):\n",
    "                    potential_headings.append(line.strip())\n",
    "        \n",
    "        return potential_headings[:10]  # 単純化のため最初の10項目のみ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スライド生成のためのモデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SlideSection(BaseModel):\n",
    "    title: str = Field(description=\"スライドのセクションタイトル\")\n",
    "    content: List[str] = Field(description=\"箇条書きのコンテンツリスト\")\n",
    "    notes: Optional[str] = Field(description=\"講演者ノート（オプショナル）\", default=None)\n",
    "\n",
    "class SlidePresentation(BaseModel):\n",
    "    title: str = Field(description=\"プレゼンテーションのタイトル\")\n",
    "    subtitle: Optional[str] = Field(description=\"サブタイトル（オプショナル）\", default=None)\n",
    "    sections: List[SlideSection] = Field(description=\"プレゼンテーションのセクション\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraphワークフローのスケッチ\n",
    "\n",
    "次のコードは実際に実行できる完全な実装ではなく、LangGraphを使ったワークフローの基本的な構造を示すスケッチです。詳細な実装は次のノートブックで行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_slide_generation_workflow():\n",
    "    # ノードの定義\n",
    "    \n",
    "    @lg.node\n",
    "    def extract_content_structure(state):\n",
    "        \"\"\"ドキュメントから主要な構造と内容を抽出\"\"\"\n",
    "        # 実装は次のノートブックで\n",
    "        return {\"content_structure\": \"構造の抽出結果\", **state}\n",
    "    \n",
    "    @lg.node\n",
    "    def generate_slide_outline(state):\n",
    "        \"\"\"抽出された構造からスライドアウトラインを生成\"\"\"\n",
    "        # 実装は次のノートブックで\n",
    "        return {\"slide_outline\": \"アウトラインの生成結果\", **state}\n",
    "    \n",
    "    @lg.node\n",
    "    def generate_detailed_slides(state):\n",
    "        \"\"\"アウトラインから詳細なスライド内容を生成\"\"\"\n",
    "        # 実装は次のノートブックで\n",
    "        return {\"slide_presentation\": \"詳細スライドの生成結果\", **state}\n",
    "    \n",
    "    @lg.node\n",
    "    def generate_html_slides(state):\n",
    "        \"\"\"構造化されたスライドデータからHTMLを生成\"\"\"\n",
    "        # 実装は次のノートブックで\n",
    "        return {\"html_output\": \"HTML出力の生成結果\", **state}\n",
    "    \n",
    "    # グラフの定義\n",
    "    workflow = lg.Graph()\n",
    "    workflow.add_node(\"extract_content_structure\", extract_content_structure)\n",
    "    workflow.add_node(\"generate_slide_outline\", generate_slide_outline)\n",
    "    workflow.add_node(\"generate_detailed_slides\", generate_detailed_slides)\n",
    "    workflow.add_node(\"generate_html_slides\", generate_html_slides)\n",
    "    \n",
    "    # エッジの定義\n",
    "    workflow.add_edge(\"extract_content_structure\", \"generate_slide_outline\")\n",
    "    workflow.add_edge(\"generate_slide_outline\", \"generate_detailed_slides\")\n",
    "    workflow.add_edge(\"generate_detailed_slides\", \"generate_html_slides\")\n",
    "    \n",
    "    # コンパイル\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# 実際のワークフローの詳細な実装は次のノートブックで行います"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次のステップ\n",
    "\n",
    "このノートブックでは、スライド生成システムの基本的な構造を紹介しました。次のノートブックでは、以下のトピックを詳しく説明します：\n",
    "\n",
    "1. より詳細なドキュメント処理と構造抽出\n",
    "2. LLMを使用したコンテンツ分析とスライド生成\n",
    "3. LangGraphを使用した完全なワークフローの実装\n",
    "4. HTMLテンプレートの活用とスタイルのカスタマイズ\n",
    "\n",
    "次のノートブック `01_document_processing.ipynb` に進んで、実際の実装を始めましょう！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}